{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cca850ad-8069-4825-8c12-85c0ad58d1b3",
   "metadata": {},
   "source": [
    "# Real-time Motion Estimation with SNNs on Neuromorphic Hardware üöÄüß†\n",
    "\n",
    "This tutorial will introduce you to real-time motion estimation using Spiking Neural Networks (SNNs) on neuromorphic hardware. We'll focus on a specific encoding scheme known as time difference encoding, which allows us to efficiently represent temporal information in spiking neural networks. By leveraging neuromorphic hardware, we can achieve real-time performance and low power consumption for motion estimation tasks.\n",
    "\n",
    "By the end of this tutorial, you will have a good understanding of real-time motion estimation using SNNs on neuromorphic hardware and how to exploit time difference encoding for efficient temporal representation using event-based data.\n",
    "\n",
    "The tutorial is based on the use of event-driven visual data exploiting time-difference encoding to encode the direction of moving objects in the scene. The publication **\"Event-Based Eccentric Motion Detection Exploiting Time Difference Encoding\"** proposes a novel approach to motion detection using an eccentric down-sampling of the visual field and the Spiking Elementary Motion Detector (sEMD) model. The system was characterized by simulated and real-world data collected with bio-inspired event-driven cameras, and successfully implemented motion detection along the four cardinal directions and diagonally. The proposed architecture is suitable for simulation on neuromorphic platforms such as SpiNNaker and offers the possibility to be easily implemented for recorded and live input data. The authors' repository containing the model and the data is available on GitHub.\n",
    "\n",
    "Click on the publication to know more! \n",
    "\n",
    "<div align=\"center\">\n",
    "<a href=\"https://www.frontiersin.org/articles/10.3389/fnins.2020.00451/full\"><img src=\"Images/articleTDE.png\" alt=\"üëâ\" width=\"500\" height=\"400\"></a>\n",
    "</div>\n",
    "\n",
    "\n",
    "## Some knowledge üìñüîç\n",
    "\n",
    "**Spiking Neural Networks (SNNs)**\n",
    "Spiking Neural Networks (SNNs) are a type of artificial neural network inspired by the information processing of biological neural networks. Unlike traditional artificial neural networks, which use continuous-valued activations, SNNs represent information through discrete, asynchronous pulses or spikes. These spikes propagate through the network, allowing for the modelling of temporal dynamics.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"Images/neuron.png\" alt=\"Alt Text\" width=\"300\" height=\"200\">\n",
    "</div>\n",
    "\n",
    "\n",
    "Key characteristics of SNNs include:\n",
    "- *Spiking Neurons*: Neurons in an SNN generate spikes in response to input stimuli. The timing and frequency of these spikes carry information.\n",
    "- *Asynchronous Computation*: SNNs operate in an event-driven fashion, where computations are triggered by spikes rather than being synchronized across all neurons.\n",
    "- *Temporal Processing*: SNNs excel at modelling and processing temporal information due to their ability to encode timing and sequence-based patterns.\n",
    "\n",
    "SNNs have shown promise in various applications, including sensory processing, pattern recognition, and robotics. They offer advantages in terms of energy efficiency, event-driven processing, and the ability to capture the dynamics of real-time data.\n",
    "\n",
    "**Neuromorphic Hardware**\n",
    "Neuromorphic hardware is specialized hardware designed to efficiently simulate and execute Spiking Neural Networks (SNNs). It aims to replicate the behaviour of biological neural systems, providing benefits such as low power consumption and real-time processing capabilities.\n",
    "\n",
    "Key features of neuromorphic hardware include:\n",
    "- *Parallelism*: Neuromorphic hardware architectures leverage parallel processing to simulate the large-scale connectivity and computational capabilities of the brain.\n",
    "- *Event-Driven Processing*: Instead of continuously processing data, neuromorphic hardware operates on an event-driven basis, only consuming power when there are relevant spikes or events.\n",
    "- *Low Power Consumption*: Neuromorphic hardware is designed to be energy-efficient, allowing for longer battery life and reduced power requirements compared to traditional computing systems.\n",
    "\n",
    "Neuromorphic hardware platforms, such as [SpiNNaker](https://apt.cs.manchester.ac.uk/projects/SpiNNaker/) and [Loihi](https://en.wikichip.org/wiki/intel/loihi) for the digital circuits; [BrainScaleS](https://example.com/brainscales_website) and [Speck](https://www.synsense.ai/products/speck/) for analog circuits, offer advantages for real-time applications and computationally demanding tasks. They provide a suitable environment for running large-scale SNN simulations, enabling researchers to explore complex neural processing with low latency and high efficiency.\n",
    "\n",
    "By combining the capabilities of SNNs and neuromorphic hardware, it is possible to achieve real-time motion estimation with low power consumption, making them an exciting field of research and development.\n",
    "\n",
    "Keep in mind that this overview provides a high-level understanding of SNNs and neuromorphic hardware. In the subsequent sections of the tutorial, we'll delve deeper into the specifics of motion estimation and time difference encoding.\n",
    "\n",
    "\n",
    "**Overview of Event-Driven Cameras**\n",
    "\n",
    "Event-driven cameras, also known as neuromorphic cameras or event-based cameras, are a type of imaging sensor that differs from traditional frame-based cameras. While traditional cameras capture a sequence of frames at a fixed frame rate, event-driven cameras work on the principle of capturing individual pixel-level changes in the scene.\n",
    "ser does not support the video tag.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"Images/0_Y-FlGt_EZc35K39M.gif\" alt=\"Alt Text\" width=\"500\" height=\"300\">\n",
    "</div>\n",
    "\n",
    "\n",
    "Here are some key points to understand about event-driven cameras:\n",
    "\n",
    "- *Event Generation:* Event-driven cameras generate events asynchronously whenever a pixel's intensity changes beyond a predefined threshold. These events, also known as \"spikes,\" provide information about the location, intensity change, and precise timing of the event occurrence.\n",
    "\n",
    "- *Asynchronous Operation:* Unlike frame-based cameras that capture images at fixed time intervals, event-driven cameras operate asynchronously, responding only to changes in the scene. This allows them to have high temporal resolution and capture fast-moving objects without motion blur.\n",
    "\n",
    "- *High Dynamic Range (HDR):* Event-driven cameras typically have a high dynamic range, meaning they can capture both bright and dark areas in a scene simultaneously. This enables them to handle challenging lighting conditions and scenes with extreme contrast.\n",
    "\n",
    "- *Low Power Consumption:* Event-driven cameras consume significantly less power compared to traditional cameras since they only activate when there is a change in the scene. The sparse output of events requires less data processing and transmission, making them suitable for low-power and resource-constrained applications.\n",
    "\n",
    "- *Applications:* Event-driven cameras find applications in various domains, including robotics, autonomous vehicles, surveillance, augmented reality, and human-machine interaction. Their ability to capture precise temporal information and handle challenging scenarios makes them valuable in real-time applications.\n",
    "\n",
    "- *Neuromorphic Hardware Integration:* Event-driven cameras are often used in conjunction with neuromorphic hardware, which is specialized hardware inspired by the structure and function of the human brain. Neuromorphic hardware can efficiently process the event data and perform tasks such as object recognition, motion estimation, and tracking.\n",
    "\n",
    "- *Advantages:* Event-driven cameras offer several advantages, such as low latency, high temporal resolution, high dynamic range, and low power consumption. They are well-suited for tasks that require fast response times, accurate temporal information, and efficient processing of sensory data.\n",
    "\n",
    "- *Limitations:* Event-driven cameras also have some limitations. They may struggle with scenes that have low-contrast or slowly changing elements since events are triggered based on intensity changes. Additionally, the processing of event data may require specialized algorithms and software frameworks.\n",
    "\n",
    "Overall, event-driven cameras provide an alternative approach to traditional frame-based cameras, offering benefits such as low latency, high temporal resolution, and low power consumption. As research and development in this field continue, event-driven cameras have the potential to revolutionize various applications that require real-time, high-speed, and low-power vision systems.\n",
    "\n",
    "Let's get started with the Tutorial üöÄ\n",
    "\n",
    "\n",
    "## Let's start with the Tutorial üß†\n",
    "\n",
    "When working with models and algorithms, it is crucial to consider the distinction between ideal and real-world data. Ideal data represents a perfectly controlled and synthetic environment that allows for precise understanding and evaluation of the model's performance. On the other hand, real-world data reflects the complexity, noise, and variability encountered in practical applications. \n",
    "\n",
    "**Ideal Data**\n",
    "\n",
    "Ideal data is often used during the initial stages of model development, where simulated or synthetic data is generated to represent specific scenarios or controlled conditions. The advantages of using ideal data include:\n",
    "\n",
    "- *Ground Truth*: In ideal data, the ground truth information is known and perfectly defined. This means that the desired outputs or labels for each input sample are provided, allowing for accurate assessment of the model's performance.\n",
    "\n",
    "- *Controlled Environment*: Simulated data offers the advantage of controlling various factors such as noise, variability, and input conditions. This control enables a detailed analysis of the model's behavior and performance under different scenarios.\n",
    "\n",
    "- *Efficiency*: Generating ideal data can be computationally efficient compared to collecting real-world data. Simulated data allows for rapid prototyping and iterative development, enabling researchers to explore different model configurations and hypotheses effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc0be644-616f-44d2-95d1-0d961fa42a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./venv/lib/python3.10/site-packages (1.24.3)\n",
      "Requirement already satisfied: matplotlib in ./venv/lib/python3.10/site-packages (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.10/site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.10/site-packages (from matplotlib) (4.39.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./venv/lib/python3.10/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in ./venv/lib/python3.10/site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.10/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./venv/lib/python3.10/site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./venv/lib/python3.10/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbe31922-7e6f-44d3-aeaa-b7abe56ca3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "# Set the camera resolution\n",
    "width = 100\n",
    "height = 100\n",
    "\n",
    "# Set the speed of the moving bar\n",
    "speed = 1  # px/ms\n",
    "period = 1/speed  # ms\n",
    "\n",
    "\n",
    "# Generate events of a white bar on a dark background\n",
    "events = {\"x\": [], \"y\": [], \"ts\": [], \"pol\": []}\n",
    "time = 0\n",
    "\n",
    "for x in np.arange(width):\n",
    "    y = np.random.choice(np.arange(0, height), size=height, replace=False)\n",
    "    ts_tmp = [np.random.uniform(time, time+period) for _ in range(height)]\n",
    "    ts = sorted(np.round(ts_tmp, decimals=2))\n",
    "    for idx in range(0, len(y)):\n",
    "        events['x'].append(x)\n",
    "        events['y'].append(y[idx])\n",
    "        events['ts'].append(ts[idx])\n",
    "        events['pol'].append(1)\n",
    "    time+=period\n",
    "\n",
    "#visualisation events\n",
    "time_window = 1 #ms\n",
    "time_tmp=time_window\n",
    "matrix_events = np.zeros((height,width))\n",
    "fig = plt.figure()\n",
    "for idx in range(0, len(events['x'])):\n",
    "    if events['ts'][idx] < time_tmp:\n",
    "        matrix_events[events['y'][idx], events['x'][idx]] = events['pol'][idx]\n",
    "    else:\n",
    "        plt.imshow(matrix_events)  # or ax.imshow(frame)\n",
    "        plt.draw()\n",
    "        plt.pause(0.0001)\n",
    "        matrix_events = np.zeros((height, width))\n",
    "        time_tmp += time_window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9adb98-1d70-4f8b-8154-b86ab2c60726",
   "metadata": {},
   "source": [
    "**Real-World Data**\n",
    "\n",
    "Real-world data represents the actual observations or measurements collected from the target application or environment where the model will be deployed. Real-world data possesses the following characteristics:\n",
    "\n",
    "- *Complexity and Variability*: Real-world data often contains noise, missing values, outliers, and unexpected patterns. It reflects the inherent complexity and diversity of the application domain, providing a more realistic assessment of the model's performance.\n",
    "\n",
    "- *Generalizability*: Testing a model on real-world data is crucial to assess its generalizability and ability to handle unseen examples. Real-world data exposes the model to the diverse conditions and scenarios that it will encounter in practical use.\n",
    "\n",
    "- *Performance Evaluation*: Evaluating a model on real-world data helps identify potential issues, limitations, and areas for improvement. It provides insights into how the model performs when facing the challenges and uncertainties of the target application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a64fdec-5651-45e3-b145-bd3ad3028c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bimvee==1.0.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5202199d-ca52-4ce7-9acd-524069a5ea3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bimvee\n",
    "import os\n",
    "from bimvee.importIitYarp import importIitYarp\n",
    "\n",
    "events = importIitYarp(filePathOrName=\"data\", tsBits=30)\n",
    "e_x = events['data']['left']['dvs']['x']\n",
    "e_y = events['data']['left']['dvs']['x']\n",
    "e_ts = events['data']['left']['dvs']['x']\n",
    "e_pol = events['data']['left']['dvs']['x']\n",
    "\n",
    "window_period = 50 #ms\n",
    "window = np.zeros(())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d487efd-05b8-44f3-87ed-6ea71c486fef",
   "metadata": {},
   "source": [
    "## Event-Based Eccentric Motion Detection Exploiting Time Difference Encoding\n",
    "\n",
    "Now, we will apply what we have learned about events and utilize it with the Time Difference Encoder (TDE). The TDE captures the timing information by measuring the time difference between events. This enables efficient representation and processing of temporal data.\n",
    "\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://www.frontiersin.org/files/Articles/520308/fnins-14-00451-HTML-r1/image_m/fnins-14-00451-g002.jpg\" alt=\"Alt Text\" width=\"300\" height=\"200\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3303cf3-0e5f-4ef1-93c6-50dc84906193",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
